from typing import Callable, Tuple

import jax.numpy as jnp
import jaxtyping as jt
from flax import linen as nn


class C2Conv(nn.Module):
    features: int
    kernel_size: Tuple[int, ...]

    @nn.compact
    def __call__(self, input):
        layer = nn.Conv(
            features=self.features,
            kernel_size=self.kernel_size,
        )
        return jnp.concatenate([layer(input), layer(-input)], axis=-1)


class C2Dense(nn.Module):
    features: int

    @nn.compact
    def __call__(self, input):
        layer = nn.Dense(
            features=self.features,
            use_bias=False,
        )

        return jnp.stack([layer(input), layer(-input)], axis=-1).squeeze()


class C2DenseTransform(nn.Module):
    features: int
    transform: Callable[[jt.Array], jt.Array]

    @nn.compact
    def __call__(self, input):
        layer = nn.Dense(
            features=self.features,
            use_bias=True,
            bias_init=nn.initializers.ones,
        )
        assert self.transform(input).shape == input.shape

        return jnp.stack(
            [layer(input), layer(self.transform(input))], axis=-1
        ).squeeze()


# class C2DenseDiscrete(nn.Module):
#     features: int
#
#     @nn.compact
#     def __call__(self, input):
#         layer = nn.Dense(
#             features=self.features,
#             use_bias=False,  # Affine Transformation is Equivariant to Action inversion
#         )
#         return jnp.stack([layer(input), layer(1 - input)], axis=-1).squeeze()


class C2DenseBinary(nn.Module):
    features: int

    @nn.compact
    def __call__(self, input):
        map_input = 2 * input - 1
        return C2Dense(features=self.features)(map_input)


if __name__ == "__main__":
    pass
