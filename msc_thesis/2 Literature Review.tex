\chapter{Literature Review}\label{Chap2}

\section{MDP Homomorphisms}

\section{Plannable Approximations to MDP Homomorphisms}
Classic MDP are required to have small state spaces. 
Deep RL with models are finiky but provide good preformance on complex tasks where only an approximation can be made. 

Action equivariant mappings are learned, if we can find a symmetry in the state/ action space we can reduce the amout of learning required. 

Bisimulation, if we have a pair of state actions, and their transition functions, we can define a distance between them over the state space. When this distance is minimised we happy. 

When a model is found with zero loss if framed in this was a homomorphism is found. 


The loss to training the networks is the distance between the latent representation of the space and the Abstract transition function. In addition the distance for the reward and the abstract reward.

I am really unsure about the negative samples??.
Contrastive learning: 
    Push positive samples 
    push negative samples 
\pagenumbering{arabic}
