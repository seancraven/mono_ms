\chapter{Literature Review}\label{chap:litreview}
% TODO Check for more papers!

\section{Deep Learning and MDP Homomorphisms}
As discussed previously In Chapter \ref{chap:background} the idea of learning a group structured MDP homomorphism is a powerful tool to make the learning problem posed by an MDP simpler. This section will discuss current attempts at exploiting MDP homomorphisms, with a focus on group-structured MDP homomorphisms.
\subsection{MDP Homomorphic Networks}\label{sec:symmetrizer}
\cite{vanderpol2020mdp} introduces the idea of performing policy-based learning that respects the symmetry of the environment, by constraining the possible policies that can be represented by a neural network.

This is achieved by using a network equivariant to Group Structured transformations on discrete action spaces. When a group-structured operation transforms the input to the network. The output policy is also transformed by this operation due to the equivariance property of the network. Thus, it exploits a group structured MDP homomorphism.

The equivariance in the deep network uses many of the same ideas as that of the G-CNNs~\cite{cohen2016group}. In that, the only requirement for a network to be equivariant to a discrete group action is that the individual layers of the network are equivariant to the group's actions. Despite the similarity, the \cite{vanderpol2020mdp} method of achieving the equivariance is quite different. The authors propose the ``symmetrizer layer". In contrast to the group convolution formulation, the symmetrizer layer achieves equivaraince by finding weight matrices that are solutions to,
\begin{equation}
	\label{eqn:symmetrizer}
	\mat{W} =S(\mat{W}) = \frac{1}{|G|}\sum_{g\in G}{\pi_g^{\mathcal{X}'}}^{-1}\mat{W}\pi_g^{\mathcal{X}}
\end{equation}
Where if $f(\vec{x}) = \mat{W}\vec{x}$, $f: \mathcal{X} \rightarrow {\mathcal{X}'}$ and $\pi_g^{\mathcal{X}}$ is the representation of $g$ in $\mathcal{X}$, then $\pi_g^{\mathcal{X}'}$ is the representation of $g$ in $\mathcal{X}'$. In order to find such linear systems of equations in a general manner for a group $G$, the authors sample many matrices randomly from the space of all possible matrices of that size $\mathcal{W}_{total}$. Then apply the symmetrizer operation, $S$, to all the sampled matrices.


Because the symmetrizer operation is linear, $\mathcal{W}$ is a set of solutions to the linear equation defined by the symmetrizer equation~\ref{eqn:symmetrizer}. To find this, many weight matrices are vectorised and stacked. This forms a new matrix, in which the singular value decomposition's basis vectors are orthogonal vectors of the equivariant subspace! These vectors $\{\mat{V}_i\}$ are all solutions to the above equation~\ref{eqn:symmetrizer}. As such, any linear combination of them is also a solution. Using the first $r$ vectors of the SVD, where $r= \text{rank}(\mathcal{W})$. An equivariant layer can b3 formed by,

\begin{equation}
	\mat{W} = \sum_{i=1}^{r}{\alpha_i\mat{V}_i}
\end{equation}

These layers have interesting properties; the size of the subspace defines how many parameters the symmetrizer net has. There is no current closed-form solution to the question of how many parameters each layer will have.

This scheme of producing equivariant layers has some notable upsides, in that you only need to know how to transform the input and output of the layer, $\mat{W} \rightarrow \mat{W}\pi_g^\mathcal{X}$ and $\mat{W} \rightarrow {\pi_g^{\mathcal{X}'}}^{-1} \mat{W}$ respectively. However, the scheme does require expensive SVD calculations. In addition it requires the sampling of many matrices, which is expensive, but this only needs to be done once at the start of training for each layer. As mentioned earlier, there is no closed-form solution to how many parameters the matrices will have, and as such, the number of parameters in the network is not known until the SVD is performed.

The larger problems with this approach are that the homomorphisms must be exact and known priori. This limits the possible scenarios in which this approach can be used, as it is not always possible to know the exact symmetry. In addition, in many cases, generalizing to continuous action spaces is impossible with the current methodology.


\subsection{Group Equivariant Deep Reinforcement Leanrning}
In much the same vein as that of \cite{vanderpol2020mdp}, \cite{mondal2020group} proposes a method of exploiting equivariant networks for Deep RL, in comparison to \cite{vanderpol2020mdp}, they propose using G-CNNs \cite{cohen2016group} to achieve equivariance, rather than the symmetrizer layer. In contrast to learning a policy, they have a network structure such that the states are mapped to an equivariant Latent space of dimension 256. This results in a network architecture that can be thought of as an equivariant embedding function, $f(s)$, and a Q-Value function, $Q(s_{eqv}, a)$, acting on this space.
\begin{align}
	f: & \mathcal{S} \rightarrow \mathcal{S}_{eqv}      \\
	Q: & \mathcal{S}_{eqv} \rightarrow \mathbb{R}^{|A|}
\end{align}

One of the key downsides of this is that it doesn't exploit the MDP's homomorphism, which also exists in the action space. Despite this \cite{mondal2020group} still demonstrate an improvement in sample efficiency over two baselines of DDQN~\cite{van2016deep} and DQN~\cite{mnih2013playing} in Snake. However, they only see a minor improvement in sample efficiency in Pacman, which also possesses the same $C_4$ group symmetry.

\subsection{$\mathbf{\text{SO}(2)}$ Equivariant Reinforcement Learning}

In much the same way as the above methods,~\cite{wang2022so2} also exploit the equivariance property in the context of robotic control in the PyBullet suite\cite{coumans2021}. They use steerable G-CNNs\cite{weiler2019general}, which provide $\text{SE}(2)$ equivariance in robotic control environments. In contrast to previous papers where finite groups were exploited, the continuous Lie group $\text{SO}(2)$ constrains the degrees of freedom of the problem much more than the finite groups. They can achieve impressive improvements in sample efficiency and robustness over conventional DQN and SAC methods.

In drawer-opening, block pulling, and object Picking experiments, the equivariant networks outperform all other DQN methods. When applied to the SAC formulation, In Object picking and Drawer Opening are the only methods to solve the task.

Further, they perform tests on more difficult robotic control tasks where demonstrations are provided; this enables the agents to tackle more complex tasks such as block stacking and house building, in which they are the only successful agents again. Due to the ability to generalise to states not seen in the demonstrations but that are in the orbit of the states seen in the demonstrations.


\section{Learning Models}
In the context of exploiting symmetries in RL, there is also the opportunity to build world models learned from experience. Two possible approaches are learning a group equivariant world model. In this case, the world model has the same state action space as the original MDP. However, the equivariance inductive bias may improve the sample efficiency of learning the world model and introduces better generalization for the agent. In some ways, this is the model-based extension of the equivariant model-free methods discussed above.

An alternative approach is to learn a group invariant model. In this case, the MDP homomorphism is used to transform the state action pairs in the same orbit to a single state action pair. This produces a world model that is "simpler" than the original MDP~\ref{fig:invariant_world_model}. From the policy learned in the world model, the policy can be lifted back to the original MDP. An example of a similar strategy is that of approximate MDP Homomorphisms.
\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\filldraw[black, rounded corners] (-5, 0) rectangle (-3.5, 0.4);
		\draw[black, thick] (-5 + 0.75, 0.2) -- (-3, 1.5);
		\draw[<-, red, ultra thick] (-5.5 , 0.2) -- ( -5 + 0.75, 0.2);
		%%%
		\filldraw[black, rounded corners] (5, 0) rectangle (5 - 1.5, 0.4);
		\draw[black, thick] (5 - 0.75, 0.2) -- (5 - 2, 1.5);
		\draw[<-, red, ultra thick] (5.5 , 0.2) -- ( 5 - 0.75, 0.2);
		%%%
		\draw[->, ultra thick] (-3, -.5) -- (-1.5, -1.5) node [sloped, midway, above] {$f_{model}$};
		\draw[->, ultra thick] (3, -.5) -- (1.5, -1.5) node [sloped, midway, above] {$f_{model}$};
		%%%
		\draw[->, ultra thick] (-1, 0.5) -- (1, 0.5) node [midway, above] {$\ell_g$};
		%%%
		\filldraw[black, rounded corners] (-.75, -2.4) rectangle (.75, -2);
		\draw[->, red, ultra thick] (-0 , -2.2) -- ( -1.25, -2.2);
		\draw[black, thick] (0, -2.2) -- (1.25,  -.9);

	\end{tikzpicture}
	\caption{Diagram of the MDP Homomorphism created by a world model, that is invariant to $\ell_g$,  the group actions .}
	\label{fig:invariant_world_model}
\end{figure}
\subsection{Approximate MDP Homomorphisms}
While \cite{van2020plannable} forgo learning explicit symmetries, they learn an approximate MDP homomorphism. This approximation is exact in the case when the model fits the data perfectly.

In contrast to learning symmetries, they are looking to find a state and action embedding that is equivariant to the model, such that,
\begin{equation}
	Z(T(s, a)) = \overline{T}(Z(s), Z_s(a))
\end{equation}
Where $Z$ is the state embedding function, and $Z_s(a)$ is the action embedding function. Thus, they construct and abstract MDP $\omc{M}$, with state space $\mathcal{Z}$ and action space $\omc{A}$, which they can act in. Further, rather than using deep RL, they discretize the state action space of the abstract MDP, $Z \rightarrow \mathcal{X}$, and use value iteration, repeated application of the bellman optimality operator, to learn the values of all state-action pairs. The value function can be interpolated from here to any abstract state-action pair. Pseudocode for this process is given below. The world model for the MDP consists of four jointly trained networks, $Z$, $Z_a$, $\overline{T}$,$R_z(z)$, which are trained to minimize a bisimulation metric. A bisimulation metric measures how different the dynamics of two MDPs are. The bisimulation metric used is a squared distance between the abstract MDP and the original MDP;
\begin{equation}
	L(\theta, \phi, \xi, \psi) = \sum_{(s', s, a) \sim \tau} d(Z_\theta (s'), \overline{T}_\phi(Z_\theta(s), Z_{a_\xi}(a))) +  d(R(s'), R_{z_\psi}(Z_\theta (s))) + C(\tilde{S})
\end{equation}
Where $d(z, z') = 1/2 (z - z')^2$ is the squared distance between two quantities. Additionally, they add a contrastive term, $C(\tilde{S})$, to stop trivial embeddings. $\tilde{S}$ is a set of randomly sampled states from the trajectory. The contrastive term, with model parameter dependence suppressed, is given by,

\begin{equation}
	C(\tilde{S}) = \sum_{\tilde{s} \in \tilde{S}} \tilde{d}(Z_\theta(\tilde{s}), \overline{T}_\phi(Z(s), Z_{a_\xi}(a)))
\end{equation}
Here, $\tilde{d}(z, z') = \max(0, \epsilon - d(z, z'))$, is a distance metric that encourages the embedding to not collapse to a point.
\begin{algorithm}
	\caption{Approximate MDP Homomorphism Pseudocode}
	\begin{algorithmic}
		\State Learn $Z: \mathcal{S} \rightarrow \mathcal{Z}$, $Z_a: \mathcal{A} \rightarrow \omc{A}$
		\State Discretise $\mathcal{Z}$ to $\mathcal{X}$.
		\State $Q(x, a) \leftarrow $ Plan in $\mathcal{X}$.
		\State $Q(z, a) \leftarrow$ Interpolate $Q(x, a)$
		\State $Q(s, a) = Q(Z(s), Z_a(a))$
	\end{algorithmic}
\end{algorithm}
This procedure produces impressive results, especially when limited to very few episodes of interaction with the MDP. Compared with a REINFORCE baseline, on 100 interactions of CartPole, the episodic return is eight times higher. Additionally, across various tasks \cite{van2020plannable}, find a more structured latent space with comparable world model methods that use Encoder networks. There are, however, some limitations with the method's inability to generalize to stochastic transition dynamics, and it is not clear how the discretization would scale to larger MDPs.

\subsection{A Simple Approach To Learning State-Action Abstraction using a Learned MDP Homomorphism}
In Recent Reinforcement Learning, another approach that is related to learning an MDP homomorphism from a world model \cite{mavor2022simple} not only learns the forward transition from a given state, $T_\theta(s_t, a_t)$ but also learns the reverse transition dynamics, $B_\phi(s_{t+1}, a_t)$. Where $T$ predicts the next state $s_{t+1}$ and $B$, predicts $s_t$, the previous state. These two models are used to find state action pairs, $((s_t, a_t), (s'_t, \overline{a}))$,  that have the same next state, which in many MDPs are states with the same valueâ€”producing an MDP homomorphism between the two sets of states with equivalent effects.

If the action $a_t$ transitions the agent from $s_t$ to $s_{t+1}$, $T(s_t, a_t) = s_{t+1}$, then there may exists an action $\overline{a}$ such that,
\begin{equation}
	B(T(s_t, a_t), \overline{a}) = s_t
\end{equation}
This is the canonical action. There may not always be a canonical action, however.

In the case of MDPs, where the reward is only a function of the state and the transition dynamics are deterministic, then an MDP homomorphism can be found. By choosing a single canonical action, $\overline{a}$ and a start state, $s_t$. Equivalent effect state action pairs can be found to have the same value. Consider the set of next states $\{s_{t+1}\}$ defined by the set of actions $\{a_t\}$, such that,
\begin{equation}
	\{T(s_t, a_i)\}_{i \in |A| }= {s_{t+1}}.
\end{equation}
The the set state with the value $Q(s_t, a_j)$ for the cannonical action is given by $B(s_{t+1}, \overline{a}) = s'_t$. Thus $Q(s_t, a_j) = Q(s'_t, \overline{a})$, which defines a MDP homomorphism between the state action pairs $\{s_t, a_j\}$ and $\{s'_t, \overline{a}\}$. The $Q$ values of only one set of these states need to be learned for the agent to be able to act optimally in the MDP. This reduces the sample complexity of learning in the MDP. \cite{mavor2022simple} Show this method's effectiveness in various environments.

This technique, however, becomes non-trivial to apply in stochastic environments due to the challenge of learning stochastic transition dynamics. Further, the method is not applicable if the actions define the reward.



\section{Meta-Learning}
Meta-Learning is the process of learning to learn across multiple tasks. This is a vast field where the goals are varied. Specifically, in this project, we are looking at parameter meta-learning, where a single model, $f_\theta$, is applied to multiple different tasks, $\tau_i \sim \mathcal{T}$, drawn from a distribution.
Each task has a per task loss, $L_{\tau_i}(\theta)$, a function of the model's parameters and may change its functional form.


The Model-Agnostic Meta-Learning (MAML) algorithm~\ref{alg:maml} is the canonical example of these methods. It works by performing a gradient-based update on each task, storing the parameters,$\theta_i'$, of the model after each task's update, and then performs a gradient update on the meta loss, which is usually the sum of the per task losses $L_{\tau_i}(\theta_i')$, evaluated with their new parameters $\theta_i'$. This is then repeated till convergence. Pseudocode is given below.


\begin{algorithm}
	\caption{MAML Algorithm}
	\label{alg:maml}
	\begin{algorithmic}
		\State $\theta$ is randomly initialised
		\While{not done}
		\State Sample batch of tasks, $\tau_i \sim p(\tau)$
		\For {each task $\tau_i$}
		\State $\theta_i' \leftarrow \theta - \alpha \nabla_\theta L_{\tau_i}(F\theta)$
		\EndFor
		\State $\theta \leftarrow \theta - \beta \nabla_\theta \sum_{\tau_i \sim p(\tau)}{L_{\tau_i}(\theta_i')}$
		\EndWhile
	\end{algorithmic}
\end{algorithm}


\subsection{Meta-Learning Symmetries By Reparametrisation}
~\cite{zhou2020meta}, proposes a method to learn approximately equivariant networks, using the Model Agnostic Meta-Leaning~(MAML) framework~\cite{finn2017model}. MAML provides gradient updates to one set of parameters. However,~\cite{zhou2020meta} proposes a method of learning group convolutional layers~\ref{sec:G-CNNs}. This is achieved by breaking the model into two sets of parameters, the convolutional filters, which are the conventionally trainable weights, and then the parameter sharing scheme. This is the non-trainable part of a general G-CNN.

By reparameterising, the weights $\mat{W} \in \mathbb{R}^{m \times n}$ of a feedforward network, into a filter $v$ and a parameter sharing scheme $\mat{U}$;
\begin{equation}
	\text{vec}(\mat{W}) = \mat{U}v
\end{equation}
One can see this in the case of a group-specific $\mat{U}_g$, which represents a group convolutional layer.
An example of the equivalence of this to the G-CNN scheme is for $C_2$, where the network is equivariant to an inversion of the input,
\begin{align}
	\mat{U_{C_2}}   & \cdot v  = [\oplus_{g \in C_2} \pi_g] \cdot v, \\
	\begin{pmatrix}
		1  & 0  \\
		0  & 1  \\
		-1 & 0  \\
		0  & -1 \\
	\end{pmatrix} & \cdot
	\begin{pmatrix}
		v_1 \\
		v_2 \\
	\end{pmatrix}
	= \begin{pmatrix}
		  v_1  \\
		  v_2  \\
		  -v_1 \\
		  -v_2 \\
	  \end{pmatrix}.
\end{align}
To prove this is equivariant, we can show that the following holds,
\begin{equation}
	\begin{pmatrix}
		0 & 1 \\
		1 & 0 \\
	\end{pmatrix} \cdot \left[
		\begin{pmatrix}
			v_1  & v_2  \\
			-v_1 & -v_2 \\
		\end{pmatrix} \cdot x \right] = \begin{pmatrix}
		-v_1 x_1 - v_2 x_2 \\
		v_1 x_1 + v_2 x_2  \\
	\end{pmatrix}
	= \begin{pmatrix}
		v_1  & v_2  \\
		-v_1 & -v_2 \\
	\end{pmatrix} \cdot  -x
\end{equation}
From this insight, they propose a method of meta-learning an equivariant parameter sharing scheme $\mat{U}$, given a set of tasks $\mathcal{T}$, all possessing the same equivariance.
Using an adapted MAML framework, $v$ is trained on a per task loss $L_{\tau_i}$ and the training data and $\mat{U}$ is trained on the meta loss on the validation data; they demonstrate the ability to recover the equivariant parameter sharing scheme, for convolutional layers.


\section{Other Related Works}

Another recent development is that of \cite{rezaei2022continuous}, which proves that the MDP homomorphism results of Optimal Value equivalence are also found in the continuous action case. Further, they use this result in conjunction with DDPG\cite{lillicrap2015continuous} to learn a continuous action MDP Homomorphism, building a world model with a lax bisimulation metric that is similar to the one used in \cite{van2020plannable}. While an interesting theoretical result, the method is just a generalisation of an MDP homomorphism and does not exploit the symmetry of the MDP.

In wider machine learning, symmetry as an inductive bias is not new, and more generally, requiring equivariance to input transformation is a key concept in modern Deep Learning. Geometric Deep Learning has recently emerged as a unified formalism for deep learning on structured data, such as graphs, sets, Groups, and others.

Geometric Deep Learning is a field that provides a theoretical commonality for many successful network architectures and has unified disparate architectures \cite{bronstein2021geometric}, such as Transformers and C-NNs, under a common framework. Geometric Deep learning provides a way to generalise the different inductive biases that arise from the world. For example, the translation invariance in object detection or order permutation in Graphs.

A recent success in this field is the $\text{SE}(3)$-Transformer, \cite{fuchs2020se}, provides an equivaraint architecture, to $\text{SE}(3)$ transformations. This ensures that global rotations and translations on graphs or point clouds respond similarly. This is an important equivariance to encode when dealing with proteins and other chemical molecules. For example, this architecture was used in the Alpha Fold protein structure prediction networks~\cite{jumper2021highly}.

Nother Networks, which take inspiration from Nother's theorem, demonstrate the ability to meta-learn symbolic conservation laws from real-world data~\cite{alet2021noether}. An example of this is recovering the Hamiltonian of spring from real-world data. This is particularly impressive as the spring itself does not conserve energy due to friction, and so the network must learn the approximate conservation law. This is achieved by meta-learning a conservation loss across multiple tasks.



