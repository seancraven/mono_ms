@article{example-citation,
  abstract  = {{We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.}},
  author    = {Anne Author},
  doi       = {10.xxxx/example.example.0001},
  journal   = {Journal of Classic Examples},
  keywords  = {stuff},
  month     = jan,
  number    = {1},
  pages     = {e1001745+},
  pmcid     = {PMC3886731},
  pmid      = {24415924},
  posted-at = {1970-01-01 00:00:01},
  publisher = {Public Library of Science},
  title     = {{Example Journal Paper Title}},
  url       = {http://dx.doi.org/10.xxx/example.example.0001},
  volume    = {1},
  year      = {1970}
}

@article{gellmann1961eight,
  title        = {THE EIGHTFOLD WAY: A THEORY OF STRONG INTERACTION SYMMETRY},
  author       = {Gell-Mann, M},
  abstractnote = {A new model of the higher symmetry of elementary particles is introduced ln which the eight known baryons are treated as a supermultiplet, degenerate in the limit of unitary symmetry but split into isotopic spin multiplets by a symmetry-breaking term. The symmetry violation is sscribed phenomenologically to the mass differences. The baryons correspond to an eight-dimensional irreducible representation of the unitary group. The pion and K meson fit into a similar set of eight particles along with a predicted pseudoscalar meson X/sup o/ having I = 0. A ninth vector meson coupled to the baryon current can be accomodated natarally in the scheme. It is predicted that the eight baryons should all have the same spin and parity and that pseudoscalar and vector mesons should form octets with possible additional singlets. The mathematics of the unitary group is described by considering three fictitious leptons, nu , e/sup -/ , and mu /sup -/, which may throw light on the structure of weak interactions. (D. L.C.)},
  doi          = {10.2172/4008239},
  url          = {https://www.osti.gov/biblio/4008239},
  place        = {United States},
  year         = {1961},
  month        = {3}
}

@article{laskin2020reinforcement,
  title   = {Reinforcement learning with augmented data},
  author  = {Laskin, Misha and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {19884--19895},
  year    = {2020}
}
@article{yijion2020invariant,
  author  = {Lin, Yijiong and Huang, Jiancong and Zimmer, Matthieu and Guan, Yisheng and Rojas, Juan and Weng, Paul},
  journal = {IEEE Robotics and Automation Letters},
  title   = {Invariant Transform Experience Replay: Data Augmentation for Deep Reinforcement Learning},
  year    = {2020},
  volume  = {5},
  number  = {4},
  pages   = {6615-6622},
  doi     = {10.1109/LRA.2020.3013937}
}

@article{mondal2020group,
  title   = {Group equivariant deep reinforcement learning},
  author  = {Mondal, Arnab Kumar and Nair, Pratheeksha and Siddiqi, Kaleem},
  journal = {arXiv preprint arXiv:2007.03437},
  year    = {2020}
}
@inproceedings{cohen2016group,
  title        = {Group equivariant convolutional networks},
  author       = {Cohen, Taco and Welling, Max},
  booktitle    = {International conference on machine learning},
  pages        = {2990--2999},
  year         = {2016},
  organization = {PMLR}
}

@article{wang2022so2,
  title   = {so2-Equivariant Reinforcement Learning},
  author  = {Wang, Dian and Walters, Robin and Platt, Robert},
  journal = {arXiv preprint arXiv:2203.04439},
  year    = {2022}
}
@article{lecun1989backprop,
  author  = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal = {Neural Computation},
  title   = {Backpropagation Applied to Handwritten Zip Code Recognition},
  year    = {1989},
  volume  = {1},
  number  = {4},
  pages   = {541-551},
  doi     = {10.1162/neco.1989.1.4.541}
}
@article{vanderpol2020mdp,
  doi       = {10.48550/ARXIV.2006.16908},
  url       = {https://arxiv.org/abs/2006.16908},
  author    = {van der Pol, Elise and Worrall, Daniel E. and van Hoof, Herke and Oliehoek, Frans A. and Welling, Max},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{ravindran2003smdp,
  title  = {SMDP homomorphisms: An algebraic approach to abstraction in semi markov decision processes},
  author = {Ravindran, Balaraman},
  year   = {2003}
}
@inproceedings{ravindran2001symmetries,
  title  = {Symmetries and Model Minimization in Markov Decision Processes},
  author = {Balaraman Ravindran and Andrew G. Barto},
  year   = {2001}
}
@article{bellamn1957mdp,
  author   = {Richard Bellman},
  title    = {A Markovian Decision Process},
  journal  = {Indiana Univ. Math. J.},
  fjournal = {Indiana University Mathematics Journal},
  volume   = 6,
  year     = 1957,
  issue    = 4,
  pages    = {679--684},
  issn     = {0022-2518},
  coden    = {IUMJAB},
  mrclass  = {}
}
@inproceedings{howard1960dynamic,
  title  = {Dynamic Programming and Markov Processes},
  author = {Ronald A. Howard},
  year   = {1960}
}
@article{minh2016asynchronous,
  doi       = {10.48550/ARXIV.1602.01783},
  url       = {https://arxiv.org/abs/1602.01783},
  author    = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Asynchronous Methods for Deep Reinforcement Learning},
  publisher = {arXiv},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{mnih2013playing,
  title   = {Playing atari with deep reinforcement learning},
  author  = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal = {arXiv preprint arXiv:1312.5602},
  year    = {2013}
}
@article{lillicrap2015continuous,
  title   = {Continuous control with deep reinforcement learning},
  author  = {Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal = {arXiv preprint arXiv:1509.02971},
  year    = {2015}
}}

@misc{schulman2015highdimensional,
  doi       = {10.48550/ARXIV.1506.02438},
  url       = {https://arxiv.org/abs/1506.02438},
  author    = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  keywords  = {Machine Learning (cs.LG), Robotics (cs.RO), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  title     = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  publisher = {arXiv},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{silver2016mastering,
  title   = {Mastering the game of Go with deep neural networks and tree search},
  author  = {David Silver and Aja Huang and Christopher J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
  year    = {2016},
  url     = {http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html},
  journal = {Nature},
  pages   = {484--503},
  volume  = {529}
}
@article{silver2017mastering,
  title   = {Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author  = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal = {arXiv preprint arXiv:1712.01815},
  year    = {2017}
}
@article{hafner2023mastering,
  title   = {Mastering Diverse Domains through World Models},
  author  = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal = {arXiv preprint arXiv:2301.04104},
  year    = {2023}
}
@article{williams1992simple,
  title     = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author    = {Williams, Ronald J},
  journal   = {Reinforcement learning},
  pages     = {5--32},
  year      = {1992},
  publisher = {Springer}
}
@article{schulman2015high,
  title   = {High-dimensional continuous control using generalized advantage estimation},
  author  = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal = {arXiv preprint arXiv:1506.02438},
  year    = {2015}
}
@inproceedings{shculman2015trust,
  title     = {Trust Region Policy Optimization},
  author    = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  pages     = {1889--1897},
  year      = {2015},
  editor    = {Bach, Francis and Blei, David},
  volume    = {37},
  series    = {Proceedings of Machine Learning Research},
  address   = {Lille, France},
  month     = {07--09 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v37/schulman15.pdf},
  url       = {https://proceedings.mlr.press/v37/schulman15.html},
  abstract  = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement with little tuning of hyperparameters.}
}
@article{schulman2017proximal,
  title   = {Proximal policy optimization algorithms},
  author  = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal = {arXiv preprint arXiv:1707.06347},
  year    = {2017}
}
@inproceedings{van2016deep,
  title     = {Deep reinforcement learning with double q-learning},
  author    = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {30},
  number    = {1},
  year      = {2016}
}
@inproceedings{henderson2018deep,
  title     = {Deep reinforcement learning that matters},
  author    = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {32},
  number    = {1},
  year      = {2018}
}
@article{dulac2019challenges,
  title   = {Challenges of real-world reinforcement learning},
  author  = {Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  journal = {arXiv preprint arXiv:1904.12901},
  year    = {2019}
}